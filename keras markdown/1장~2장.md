# 1장. 딥러닝이란 무엇인가?

### 1.2.3. 커널방법

- **커널방법** : 분류 알고리즘의 한 방식
  - SVM이 가장 유명함 : 회귀뿐만 아니라 분류에도 사용 가능
  - SVM의 결정 경계 찾는 과정
    - 하나의 hyperplane으로 표현될 수 있는 새로운 고차원 표현으로 데이터 매핑
    - 초평면과 각 클래스의 가장 가까운 데이터의 포인트가 최대가 되는 결정 경계를 찾는다(**마진 최대화**)
  - **커널 함수** : 원본 공간에 있는 두 데이터 포인트를 명시적으로 새로운 표현으로 변환하지 않고, 타깃 표현 공간에 위치했을 때의 거리를 매핑해 주는 계산 가능 연산임.



### 1.2.4. 결정트리, 랜덤 포레스트, 그래디언트 부스팅

- **결정트리** : 일반적인 형태의 트리로 분류하는 모델
- **랜덤 포레스트** : 서로다른 결정트리 많이 만들고, 출력 앙상블하는 방법
- **그래디언트 부스팅** : 랜덤 포레스트와 비슷하지만, 약한 예측모델인 트리를 앙상블하고, 모델이 놓친 데이터를 보완하는 새로운 모델을 만듬



# 2장. 신경망의 수학적 구성요소

[2.1. 신경망과의 첫만남](#2.1.-신경망과의-첫만남)

[2.2. 신경망을 위한 데이터 표현](#2.2.-신경망을-위한-데이터-표현)

[2.3. 신경망의 톱니바퀴:텐서 연산](#2.3.-신경망의-톱니바퀴:텐서-연산)

[2.4. 신경망의 엔진:그래디언트 기반 최적화](#2.4.-신경망의-엔진:그래디언트-기반-최적화)

[2.5. 첫 번째 예제 다시 살펴보기](#2.5.-첫-번째-예제-다시-살펴보기)

---

### 2.1. 신경망과의 첫만남

#### MNIST 데이터 살펴보기

```python
from keras.datasets import mnist

#데이터 불러오기
(train_img, train_lab), (test_img, test_lab) = mnist.load_data()

#데이터 모양 확인
>>>train_images.shape
(60000,28,28)
>>>len(train_labels)
60000
>>> train_labels
array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)
```

#### 신경망 구조

```python
from keras import models
from keras import layers

network = models.Sequential()

network.add(layers.Dense(512, activation = 'relu', input_shape = (28*28,)))

#10개의 확률 점수가 들어있는 배열을 반환하는 softmax 함수층.
network.add(layers.Dense(10, activation = 'softmax'))
```

> 2개의 **Dense**층으로 이루어져 있음 (or fully connected layer)



#### 컴파일 단계 

- 컴파일에 포함될 세 가지가 필요함
  - **손실함수 (loss function)** : 훈련데이터에서 신경망의 성능을 측정하는 방법, 네트워크가 옳은 방향으로 학습될 수 있도록 함
  - **옵티마이저 (optimizer)** : 입력된 데이터와 손실 함수를 기반으로 네트워크를 업데이트하는 메커니즘
  - **훈련과 테스트 과정을 모니터링할 지표** : 아래 예시에서는 정확도만 고려

```python
network.compile(optimizer = 'rmsprop',
               loss = 'categorical_crossentropy',
               metrics = ['accuracy'])
```



#### 이미지 데이터 준비

- 훈련 시작 전 데이터를 네트워크 크기에 맞게 0~1사이로 스케일 조정해야함
- mnist는 60000, 28, 28인데 이를 60000, 28*28 크기의 배열로 바꿔줘야함

```python
train_img = train_img.reshape((60000, 28 * 28))
train_img = train_img.astype('float32')/255

test_img = test_img.reshape((10000, 28 * 28))
test_img = test_img.astype('float32')/255
```



#### 레이블 준비

- 이후 레이블을 범주형으로 인코딩 해야함 (3장에서 자세히)

```python
from keras.utils import to_categorical

train_lab = to_categorical(train_lab)
test_lab = to_categorical(test_lab)
```



#### fit method 를 활용하여 학습 시작

```python
>>> network.fit(train_img, train_lab, epochs = 5, batch_size = 128)
Epoch 1/5
60000/60000 [==============================] - 3s 43us/step - loss: 0.2552 - acc: 0.9261
Epoch 2/5
60000/60000 [==============================] - 1s 20us/step - loss: 0.1038 - acc: 0.9689
Epoch 3/5
60000/60000 [==============================] - 1s 11us/step - loss: 0.0683 - acc: 0.9795
Epoch 4/5
60000/60000 [==============================] - 1s 18us/step - loss: 0.0492 - acc: 0.9850
Epoch 5/5
60000/60000 [==============================] - 2s 28us/step - loss: 0.0367 - acc: 0.9890
```



#### TEST set 에서 모델 확인

```python
>>> test_loss, test_acc = network.evaluate(test_img, test_lab)
10000/10000 [==============================] - 0s 16us/step

>>> print('test_acc:', test_acc)
test_acc: 0.9787
```

---

### 2.2. 신경망을 위한 데이터 표현

**텐서란?** : 

- 데이터를 위한 컨테이너
- ex) 2D 텐서 = 행렬



##### 2.2.1 스칼라(0D Tensor)

##### 2.2.2 벡터(1D Tensor)

##### 2.2.3 행렬(2D Tensor)

##### 2.2.4 3D 텐서와 고차원 텐서

```python
>>> x = np.array([[[5, 78, 2, 34, 0],
                   [6, 79, 3, 35, 1],
                   [7, 80, 4, 36, 2]],
                  [[5, 78, 2, 34, 0],
                   [6, 79, 3, 35, 1],
                   [7, 80, 4, 36, 2]],
                  [[5, 78, 2, 34, 0],
                   [6, 79, 3, 35, 1],
                   [7, 80, 4, 36, 2]]])

>>> x.ndim
3
```



##### 2.2.5. 핵심 속성

- 축의 개수(랭크) : 넘파이의 ndim 메소드
- 크기(shape) 
- 데이터 타입 : 보통 float32, uint8, float64 등이 있음

```python
digit = train_images[4]
import matplotlib.pyplot as plt

plt.imshow(digit, cmap=plt.cm.binary)
plt.show()
```

를 통해 이미지를 볼 수 있음.



##### 2.2.6. 넘파이로 텐서 조작하기

- 슬라이싱을 통해 이미지 조작하면됨.



##### 2.2.7. 배치 데이터

- **Batch(배치)** : 슬라이싱을 통해 데이터의 전체가 아닌 일부를 자름.



### 2.3. 신경망의 톱니바퀴:텐서 연산





### 2.4. 신경망의 엔진:그래디언트 기반 최적화

### 2.5. 첫 번째 예제 다시 살펴보기